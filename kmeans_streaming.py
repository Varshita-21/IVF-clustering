# -*- coding: utf-8 -*-
"""kmeans_streaming.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P3RaFg3vS3s4DpRCF6-sk72L8Vl5vzy9
"""

!apt install libomp-dev
!pip install faiss-cpu

import faiss
import numpy as np
from sklearn.cluster import KMeans,BisectingKMeans
import shutil
import urllib.request as request
from contextlib import closing
import tarfile
import time
import pickle
from google.colab import files
import numpy as np
import faiss
from tqdm import tqdm
import os
import numpy as np
from sklearn.cluster import KMeans
import tarfile
from urllib import request
import shutil

with closing(request.urlopen('ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz')) as r:
    with open('sift.tar.gz', 'wb') as f:
        shutil.copyfileobj(r, f)

tar = tarfile.open('sift.tar.gz', "r:gz")
tar.extractall()

def read_fvecs(fp):
    a = np.fromfile(fp, dtype='int32')
    d = a[0]
    return a.reshape(-1, d + 1)[:, 1:].copy().view('float32')

base_file_path = "sift/sift_base.fvecs"
query_file_path = "sift/sift_query.fvecs"

def streaming_data_generator(file_path, batch_size=1000, dimension=128):
    with open(file_path, 'rb') as file:
        while True:
            batch = np.fromfile(file, dtype=np.float32, count=batch_size * dimension)
            if not batch.size:
                break
            num_elements = batch.size // dimension
            batch = batch[:num_elements * dimension]
            batch = batch.reshape(-1, dimension)
            yield batch

def perform_kmeans_clustering(data, k_clusters=100):
    kmeans = KMeans(n_clusters=k_clusters)
    kmeans.fit(data)
    return kmeans.cluster_centers_.astype('float32')

import numpy as np
import pickle

# Assuming streaming_data_generator() and perform_kmeans_clustering() are defined elsewhere

batch_size = 1000
dimension = 128
centroids = None
labels = None

# Iterate over batches of streaming data
for batch in streaming_data_generator(base_file_path, batch_size=batch_size, dimension=dimension):
    # Perform K-Means clustering on the current batch
    batch_centroids, batch_labels = perform_kmeans_clustering(batch)

    # Concatenate batch centroids with existing centroids
    if centroids is None:
        centroids = batch_centroids
        labels = batch_labels
    else:
        centroids = np.concatenate([centroids, batch_centroids], axis=0)
        labels = np.concatenate([labels, batch_labels])

# Save the centroids and label

kmeans_model = (centroids, labels)
with open('kmeans_streaming.pkl', 'wb') as f:
    pickle.dump(kmeans_streaming, f)

centroids.shape

import numpy as np

# Assuming centroids, labels, and centroid_index are defined elsewhere

# Loop over each centroid to compute the quantization error for each cluster
quantization_errors = []
for centroid_index, centroid in enumerate(centroids):
    # Get the indices of data points assigned to this cluster
    cluster_indices = np.where(labels == centroid_index)[0]
    # Check if cluster_indices are within bounds before accessing centroids
    valid_indices = cluster_indices[cluster_indices < len(centroids)]
    # Calculate the distance between the centroid and all data points in this cluster
    distances_to_centroid = np.linalg.norm(centroids[valid_indices] - centroid, axis=1)
    # Compute the quantization error for this cluster
    cluster_quantization_error = np.mean(distances_to_centroid)
    # Append the quantization error to the list
    quantization_errors.append(cluster_quantization_error)

# Calculate the overall quantization error by averaging the quantization errors for all clusters
overall_quantization_error = np.mean(quantization_errors)

print("Quantization Error:", overall_quantization_error)

# Create a Faiss index
quantizer = faiss.IndexFlatL2(128)  # Assuming L2 distance for quantizer
k = centroids.shape[0]  # Number of clusters
index = faiss.IndexIVFFlat(quantizer, 128, k, faiss.METRIC_L2)
index.train(centroids)

import numpy as np

# Assuming centroids and labels are defined elsewhere

# Calculate the number of clusters
num_clusters = len(np.unique(labels))

# Initialize an array to store the size of each cluster
cluster_sizes = np.zeros(num_clusters)

# Loop over each cluster to compute its size
for i in range(num_clusters):
    # Count the number of data points assigned to this cluster
    cluster_sizes[i] = np.sum(labels == i)

# Calculate the standard deviation of cluster sizes
cluster_imbalance = np.std(cluster_sizes)

print("Cluster Imbalance:", cluster_imbalance)

index.add(centroids)

# Read query vectors
query_data = read_fvecs(query_file_path)

# Perform search using Faiss index
D, I = index.search(query_data, k=1)

print("Indices of nearest centroids:", I)

