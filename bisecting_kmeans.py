# -*- coding: utf-8 -*-
"""Bisecting_KMeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M-flQ1Idr4tLHIfpTcAbgc7M_32WuLvX
"""

#!pip install k-means-constrained
!apt install libomp-dev
!pip install faiss-cpu

import faiss
import numpy as np
from sklearn.cluster import KMeans,BisectingKMeans
import shutil
import urllib.request as request
from contextlib import closing
import tarfile
import time
import pickle
from google.colab import files

with closing(request.urlopen('ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz')) as r:
    with open('sift.tar.gz', 'wb') as f:
        shutil.copyfileobj(r, f)

tar = tarfile.open('sift.tar.gz', "r:gz")
tar.extractall()

def read_fvecs(fp):
    a = np.fromfile(fp, dtype='int32')
    d = a[0]
    return a.reshape(-1, d + 1)[:, 1:].copy().view('float32')

# data we will search through
xb = read_fvecs('sift/sift_base.fvecs')  # 1M samples
# also get some query vectors to search with
xq = read_fvecs('sift/sift_query.fvecs')
# take just one query (there are many in sift_learn.fvecs)
#xq = xq[1].reshape(1, xq.shape[1])

#max = 7813, min = 7812

# Commented out IPython magic to ensure Python compatibility.
# %%time
# k = 128  # Number of clusters
# kmeans = BisectingKMeans(n_clusters=k)
# kmeans.fit(xb)
# #with open('bikmeans_sklearn.pkl','wb') as f:
# #    pickle.dump(kmeans, f)
# centroids = kmeans.cluster_centers_.astype('float32')
# quantizer = faiss.IndexFlatL2(128)  # Assuming L2 distance for quantizer
# index = faiss.IndexIVFFlat(quantizer, 128, k, faiss.METRIC_L2)
# index.train(centroids)
# index.add(xb)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# index.nprobe = 50
# tt = 0
# index_result = []
# for i in range(xq.shape[0]):
#   q = xq[i].reshape(1, xq.shape[1])
#   k = 10  # Number of nearest neighbors to search for
#   start_time = time.time()
#   distances, indices = index.search(q, k)
#   end = time.time()
#   tt = tt + (end - start_time)
#   index_result.append(indices)

tt

# Query the index to find the nearest centroid for each data point
quantization_errors = []
labels = kmeans.labels_
# Loop over each centroid to compute the quantization error for each cluster
for centroid_index, centroid in enumerate(centroids):
    # Get the indices of data points assigned to this cluster
    cluster_indices = np.where(labels == centroid_index)[0]
    # Calculate the distance between the centroid and all data points in this cluster
    distances_to_centroid = np.linalg.norm(xb[cluster_indices] - centroid, axis=1)
    # Compute the quantization error for this cluster
    cluster_quantization_error = np.mean(distances_to_centroid)

    # Append the quantization error to the list
    quantization_errors.append(cluster_quantization_error)
# Calculate the overall quantization error by averaging the quantization errors for all clusters
overall_quantization_error = np.mean(quantization_errors)

# Cluster Imbalance
cluster_assignments = kmeans.labels_
cluster_sizes = np.bincount(cluster_assignments)
cluster_imbalance = np.std(cluster_sizes)

print("Quantization Error:", overall_quantization_error)
print("Cluster Imbalance:", cluster_imbalance)

index1 = faiss.IndexFlatL2(128)
index1.add(xb)
index_result1 = []
for i in range(xq.shape[0]):
  q = xq[i].reshape(1, xq.shape[1])
  k = 10  # Number of nearest neighbors to search for
  distances, indices = index1.search(q, k)
  index_result1.append(indices)

arr = 0
for i in range(len(index_result)):
  arr = arr + sum(np.in1d(index_result[i],index_result1[i]))/10
acc = arr/len(index_result)
print(acc)

